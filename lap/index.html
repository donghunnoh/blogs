<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Latent Adaptive Planner for Dynamic Manipulation</title>

<!-- MathJax for rendering mathematical formulas -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>

<style>
* {
margin: 0;
padding: 0;
box-sizing: border-box;
}

body {
font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
line-height: 1.6;
color: #333;
background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
min-height: 100vh;
}

.container {
max-width: 1200px;
margin: 0 auto;
background: white;
box-shadow: 0 0 30px rgba(0,0,0,0.1);
min-height: 100vh;
}

.header {
background: white;
color: #333;
padding: 3rem 2rem;
text-align: center;
position: relative;
overflow: hidden;
border-bottom: 3px solid #2774AE;
}

.header-content {
position: relative;
z-index: 2;
}

.title {
font-size: 2.8rem;
font-weight: bold;
margin-bottom: 1rem;
color: #2774AE;
}

.authors {
font-size: 1.2rem;
margin-bottom: 0.5rem;
color: #555;
}

.affiliations {
font-size: 1rem;
color: #666;
margin-bottom: 2rem;
}

.institution-logos {
display: flex;
justify-content: center;
align-items: center;
gap: 2rem;
margin-top: 1rem;
}

.institution-logos img {
height: 60px;
opacity: 0.8;
}

.links {
display: flex;
justify-content: center;
gap: 1rem;
margin-top: 2rem;
flex-wrap: wrap;
}

.btn {
background: #2774AE;
color: white;
padding: 0.8rem 1.5rem;
text-decoration: none;
border-radius: 25px;
font-weight: 600;
transition: all 0.3s ease;
border: 2px solid #2774AE;
}

.btn:hover {
background: #1e5a96;
border-color: #1e5a96;
transform: translateY(-2px);
box-shadow: 0 5px 15px rgba(39, 116, 174, 0.3);
}

.content {
padding: 2rem;
}

.video-section {
text-align: center;
margin: 3rem 0;
padding: 2rem;
background: #f8f9fa;
border-radius: 15px;
box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);
}

.video-title {
font-size: 1.5rem;
font-weight: bold;
margin-bottom: 1.5rem;
color: #2774AE;
}

.video-wrapper {
position: relative;
padding-bottom: 56.25%;
height: 0;
overflow: hidden;
border-radius: 10px;
box-shadow: 0 10px 30px rgba(0,0,0,0.2);
}

.video-wrapper iframe {
position: absolute;
top: 0;
left: 0;
width: 100%;
height: 100%;
}

.experiments-section {
margin: 2rem 0;
}

.method-section {
background: white;
border-radius: 15px;
overflow: hidden;
box-shadow: 0 5px 20px rgba(0,0,0,0.1);
margin-bottom: 3rem;
}

.method-header {
padding: 1.5rem;
font-size: 1.5rem;
font-weight: bold;
text-align: center;
color: white;
}

.model-based { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }
.bc { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); }
.diffusion { background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); }
.lap { background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%); }

.videos-grid {
display: grid;
grid-template-columns: repeat(3, 1fr);
gap: 1.5rem;
padding: 2rem;
}

.video-item {
display: flex;
flex-direction: column;
}

.video-container {
aspect-ratio: 16/9;
background: #000;
border-radius: 8px;
overflow: hidden;
position: relative;
margin-bottom: 0.5rem;
}

.video-container video {
width: 100%;
height: 100%;
object-fit: contain;
}

.video-caption {
text-align: center;
font-size: 0.9rem;
color: #666;
padding: 0.5rem 0;
}

.video-placeholder {
width: 100%;
height: 100%;
display: flex;
align-items: center;
justify-content: center;
background: linear-gradient(45deg, #f0f0f0 25%, transparent 25%), 
      linear-gradient(-45deg, #f0f0f0 25%, transparent 25%), 
      linear-gradient(45deg, transparent 75%, #f0f0f0 75%), 
      linear-gradient(-45deg, transparent 75%, #f0f0f0 75%);
background-size: 20px 20px;
background-position: 0 0, 0 10px, 10px -10px, -10px 0px;
color: #666;
font-size: 0.9rem;
text-align: center;
opacity: 0.7;
}

.section {
margin: 3rem 0;
padding: 2rem 0;
}

.section-title {
font-size: 2rem;
font-weight: bold;
margin-bottom: 1.5rem;
color: #2774AE;
border-bottom: 3px solid #FFD100;
padding-bottom: 0.5rem;
}

.subsection-title {
font-size: 1.3rem;
font-weight: bold;
margin: 2rem 0 1rem 0;
color: #444;
}

.text-content {
font-size: 1.1rem;
line-height: 1.8;
margin-bottom: 1.5rem;
text-align: justify;
}

.highlight {
background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
padding: 1.5rem;
border-radius: 10px;
border-left: 4px solid #2774AE;
margin: 2rem 0;
}

.features-grid {
display: grid;
grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
gap: 1.5rem;
margin: 2rem 0;
}

.feature-card {
background: white;
padding: 1.5rem;
border-radius: 10px;
box-shadow: 0 3px 15px rgba(0,0,0,0.1);
border-top: 3px solid #2774AE;
}

.feature-card h3 {
color: #2774AE;
margin-bottom: 1rem;
}

.results-table {
overflow-x: auto;
margin: 2rem 0;
}

table {
width: 100%;
border-collapse: collapse;
background: white;
border-radius: 10px;
overflow: hidden;
box-shadow: 0 5px 20px rgba(0,0,0,0.1);
}

th, td {
padding: 1rem;
text-align: center;
border-bottom: 1px solid #eee;
}

th {
background: #2774AE;
color: white;
font-weight: bold;
}

.best-result {
background: #e8f5e8;
font-weight: bold;
color: #2d5d2d;
}

.image-figure {
text-align: center;
margin: 2rem 0;
}

.image-figure img {
max-width: 100%;
height: auto;
border-radius: 10px;
box-shadow: 0 5px 20px rgba(0,0,0,0.1);
}

.image-caption {
margin-top: 1rem;
font-style: italic;
color: #666;
font-size: 1.2rem;
}

.two-column {
display: grid;
grid-template-columns: 1fr 1fr;
gap: 2rem;
align-items: center;
margin: 2rem 0;
}

.citation {
background: #f8f9fa;
padding: 1.5rem;
border-radius: 10px;
border-left: 4px solid #2774AE;
font-family: 'Courier New', monospace;
font-size: 0.9rem;
margin: 2rem 0;
position: relative;
}

.copy-btn {
position: absolute;
top: 10px;
right: 10px;
background: #2774AE;
color: white;
border: none;
padding: 0.5rem 1rem;
border-radius: 5px;
cursor: pointer;
font-size: 0.8rem;
}

.copy-btn:hover {
background: #1e5a96;
}

.equation-box {
background: #f8f9fa;
padding: 1.5rem;
border-radius: 10px;
margin: 1rem 0;
font-size: 1.1rem;
overflow-x: auto;
}

@media (max-width: 1024px) {
.videos-grid {
grid-template-columns: repeat(2, 1fr);
}
}

@media (max-width: 768px) {
.title {
font-size: 2rem;
}

.authors {
font-size: 1rem;
}

.content {
padding: 1rem;
}

.two-column {
grid-template-columns: 1fr;
}

.institution-logos {
flex-direction: column;
gap: 1rem;
}

.videos-grid {
grid-template-columns: 1fr;
}
}
</style>
</head>
<body>
<div class="container">
<header class="header">
<div class="header-content">
<h1 class="title">Latent Adaptive Planner for Dynamic Manipulation</h1>
<div class="authors">
  Donghun Noh<sup>1‚Ä†</sup>, Deqian Kong<sup>1,2‚Ä†</sup>, Minglu Zhao<sup>1</sup>, Andrew Lizarraga<sup>1</sup>, 
  Jianwen Xie<sup>2</sup>, Ying Nian Wu<sup>1‚Ä°</sup>, Dennis Hong<sup>1‚Ä°</sup>
</div>
<div class="affiliations">
  <sup>1</sup>UCLA, <sup>2</sup>Lambda Inc. 
  <br><sup>‚Ä†</sup>Equal Contribution, <sup>‚Ä°</sup>Equal Advising
</div>

<div class="institution-logos">
  <img src="images/ucla.png" alt="UCLA">
  <img src="images/romela.png" alt="ROMELA">
  <img src="images/lambda.png" alt="Lambda">
</div>

<div class="links">
  <a href="#" class="btn">üìÑ Paper (arXiv)</a>
  <a href="#" class="btn" style="opacity: 0.7; cursor: not-allowed;">üíª Code (Coming Soon)</a>
</div>
</div>
</header>

<main class="content">
<!-- Spotlight Video Section -->
<div class="video-section">
  <h2 class="video-title">üé¨ 1-Minute Spotlight Video</h2>
  <div class="video-wrapper">
    <iframe src="https://www.youtube.com/embed/B9eLiZ1-NRQ" 
            title="LAP Spotlight Video" 
            frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
            allowfullscreen>
    </iframe>
  </div>
  </div>

<!-- TL;DR Section -->
<div class="highlight">
<h2 style="margin-top: 0; color: #2774AE;">üéØ TL;DR</h2>
<p>We propose <strong>Latent Adaptive Planner (LAP)</strong>, a trajectory-level latent variable policy for dynamic manipulation that learns effectively from human demonstration videos and achieves real-time adaptation through variational replanning. LAP demonstrates <strong>5-6.5√ó lower energy consumption</strong> compared to model-based methods while maintaining a near-perfect 98% success rate.</p>
</div>

<div class="lap-highlight">
  <div class="method-header lap">LAP Box Catching</div>
  <div class="videos-grid">
      <div class="video-item">
        <div class="video-container">
          <video controls muted playsinline loading="lazy">
              <source src="videos/lap-robot-A-box-1.mp4" type="video/mp4">
          </video>
        </div>
          <div class="video-caption">Robot A - Box 1</div>
      </div>
      <div class="video-item">
          <div class="video-container">
            <video controls muted playsinline loading="lazy">
                <source src="videos/lap-robot-A-box-2.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-caption">Robot A - Box 2</div>
      </div>
      <div class="video-item">
          <div class="video-container">
              <video controls muted playsinline loading="lazy">
                <source src="videos/lap-robot-A-box-3.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-caption">Robot A - Box 3</div>
      </div>
      <div class="video-item">
          <div class="video-container">
              <video controls muted playsinline loading="lazy">
                <source src="videos/lap-robot-B-box-1.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-caption">Robot B - Box 1</div>
      </div>
      <div class="video-item">
          <div class="video-container">
              <video controls muted playsinline loading="lazy">
                <source src="videos/lap-robot-B-box-2.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-caption">Robot B - Box 2</div>
      </div>
      <div class="video-item">
          <div class="video-container">
              <video controls muted playsinline loading="lazy">
                <source src="videos/lap-robot-B-box-3.mp4" type="video/mp4">
              </video>
          </div>
          <div class="video-caption">Robot B - Box 3</div>
      </div>
  </div>
</div>

<!-- Introduction Section -->
<section class="section">
<h2 class="section-title">üìñ Introduction</h2>

<div class="text-content">
  We present <strong>Latent Adaptive Planner (LAP)</strong>, a trajectory-level latent-variable policy for dynamic manipulation that addresses three fundamental challenges:
</div>

<div class="features-grid">
  <div class="feature-card">
      <h3>üéì Learning from Human Demonstrations</h3>
      <p>Effectively learns from human demonstration videos through model-based data regeneration that bridges the embodiment gap between humans and robots</p>
  </div>
  
  <div class="feature-card">
      <h3>‚ö° Real-Time Adaptation</h3>
      <p>Achieves real-time adaptation through variational replanning with hierarchical control: 30Hz planning, 100Hz policy, and 1kHz MPC</p>
  </div>
  
  <div class="feature-card">
      <h3>üîÑ Cross-Platform Transfer</h3>
      <p>Enables cross-platform transfer via model-based data regeneration, validating on two different robot embodiments</p>
  </div>
</div>

<div class="text-content" style="margin-top: 2rem;">
  The key innovation of LAP lies in combining classical variational Bayes learning with efficient variational replanning at inference time, enabling the robot to continuously refine its latent plans based on observed trajectories while maintaining smooth, human-like motion patterns.
</div>
</section>

<!-- Dynamic Manipulation Section -->
<section class="section">
<h2 class="section-title">üéØ Dynamic Manipulation</h2>

<div class="text-content">
  <strong>Dynamic manipulation</strong> refers to the exploitation of object dynamics for real-time robotic control that goes beyond traditional quasi-static grasping approaches. Unlike manipulation tasks where objects remain nearly stationary, dynamic manipulation requires robots to interact with objects that are actively moving, rotating, or exhibiting complex dynamics.
</div>

<div class="highlight" style="margin: 2rem 0;">
  <h3 style="color: #2774AE; margin-top: 0;">Why Box Catching?</h3>
  <p>Box catching exemplifies the challenges of dynamic manipulation:</p>
  <ul style="margin-top: 1rem; line-height: 1.8;">
      <li><strong>Unpredictable Trajectories:</strong> Human throws combined with rectangular geometry create chaotic, hard-to-predict box motions</li>
      <li><strong>Real-Time Requirements:</strong> Millisecond-scale prediction and reaction times are critical for success</li>
      <li><strong>Dual-Arm Coordination:</strong> Requires precise coordination between two robot arms under uncertainty</li>
      <li><strong>Immediate Failure Consequences:</strong> Timing errors or incorrect predictions lead to immediate task failure</li>
  </ul>
</div>

<div class="text-content">
  These characteristics make box catching an ideal testbed for evaluating dynamic manipulation capabilities, requiring the integration of perception, planning, and control at multiple time scales.
</div>
</section>

<!-- System Architecture Section -->
<section class="section">
<h2 class="section-title">üèóÔ∏è System Architecture</h2>

<div class="text-content">
  LAP employs a hierarchical control architecture that operates at three distinct frequencies, enabling both reactive responses and smooth trajectory execution:
</div>

<div class="two-column" style="margin: 2rem 0;">
  <div>
      <div class="image-figure">
          <img src="images/system_architecture.png" alt="System Architecture">
          <div class="image-caption">Complete perception-planning-control pipeline</div>
      </div>
  </div>
  <div>
      <div class="feature-card">
          <h3>Perception Module (100Hz)</h3>
          <ul style="line-height: 1.8;">
              <li><strong>Camera:</strong> ZED2 stereo camera for 3D perception</li>
              <li><strong>Segmentation:</strong> YOLOv8 trained on custom dataset for box detection and tracking</li>
              <li><strong>State Estimation:</strong> Real-time pose/size estimation of the box</li>
          </ul>
      </div>
  </div>
</div>

<div class="features-grid">
  <div class="feature-card">
      <h3>LAP Planning Layer</h3>
      <ul style="line-height: 1.8;">
          <li><strong>Latent Update (30Hz):</strong> Variational replanning updates posterior distribution over latent variables</li>
          <li><strong>Action Generation (100Hz):</strong> Transformer decoder generates control actions conditioned on latent plan</li>
      </ul>
  </div>
  
  <div class="feature-card">
      <h3>MPC Controller (1kHz)</h3>
      <ul style="line-height: 1.8;">
          <li><strong>Trajectory Smoothing:</strong> Ensures physically feasible motions</li>
          <li><strong>Constraint Enforcement:</strong> Joint limits, collision avoidance, torque limits</li>
          <li><strong>Low-Level Control:</strong> Converts high-level actions to motor commands</li>
      </ul>
  </div>
</div>

<div class="text-content" style="margin-top: 2rem;">
  This multi-rate architecture allows LAP to balance computational efficiency with responsiveness, dedicating more computation to latent plan updates while maintaining high-frequency action generation and control.
</div>
</section>

<!-- Data Regeneration Pipeline Section -->
<section class="section">
<h2 class="section-title">üîÑ Data Regeneration Pipeline</h2>

<div class="text-content">
  A critical challenge in learning from human demonstrations is the <strong>embodiment gap</strong>, which is the difference in physical structure, dynamics, and capabilities between humans and robots. Our model-based data regeneration pipeline addresses this through a three-stage process:
</div>

<div class="image-figure" style="margin: 2rem 0;">
  <img src="images/data_regeneration_concept.png" alt="Data Regeneration Pipeline">
  <div class="image-caption">Pipeline for transforming human demonstrations into robot-executable trajectories</div>
</div>

<div class="subsection-title">Stage 1: Scene State Estimation</div>
<div class="features-grid">
  <div class="feature-card">
      <h3>Box Tracking</h3>
      <p>Custom-trained YOLOv8 model detects and tracks the box in each video frame, estimating its 6D pose (position and orientation) through multi-view geometry</p>
  </div>
  
  <div class="feature-card">
      <h3>Human Pose Estimation</h3>
      <p>OpenMMLab's 2D pose estimation extracts human arm configurations from demonstration videos, providing skeletal joint positions</p>
  </div>
  
  <div class="feature-card">
      <h3>Contact Detection</h3>
      <p>Geometric reasoning between box pose/size and human hand positions identifies contact moments and forces during catching</p>
  </div>
</div>

<div class="subsection-title">Stage 2: Object-Robot Proportional Mapping</div>
<div class="text-content">
  We transform the scene from the human's reference frame to the robot's reference frame while accounting for scale differences:
</div>
<div class="equation-box">
  <strong>Frame Transformation:</strong> $^R p_{obj} = T_S^R \cdot \,^S p_{obj}$
  <br><br>
  <strong>Scale Adjustment:</strong> Scale by arm length ratio to match robot's workspace
</div>

<div class="subsection-title">Stage 3: Kinematic-Dynamic Reconstruction</div>
<div class="text-content">
  The final stage converts human arm motions into robot-executable trajectories:
</div>
<div class="features-grid">
  <div class="feature-card">
      <h3>Joint Mapping</h3>
      <p>Map human joint angles to robot joint space: $q = f_{map}(q_{human})$, accounting for kinematic differences</p>
  </div>
  
  <div class="feature-card">
      <h3>Velocity & Acceleration</h3>
      <p>Compute joint velocities and accelerations through numerical differentiation with smoothing filters</p>
  </div>
  
  <div class="feature-card">
      <h3>Inverse Dynamics</h3>
      <p>Calculate required joint torques using the robot's dynamics model to reproduce the motion</p>
  </div>
</div>

<div class="equation-box" style="text-align: center;">
  <strong>Torque Computation:</strong>
  <br><br>
  $$\tau = M(q)\ddot{q} + C(q,\dot{q})\dot{q} + G(q) + J^T F_{ext}$$
</div>

<div class="text-content">
  This regenerated data provides physically consistent robot trajectories that preserve the essential motion strategies from human demonstrations while respecting the robot's physical constraints.
</div>
</section>

<!-- LAP Model Section -->
<section class="section">
  <h2 class="section-title">üß† LAP Model</h2>
  
  <div class="text-content">
    The Latent Adaptive Planner models the joint distribution of trajectories and latent plans through a hierarchical generative process. Unlike standard behavior cloning, LAP introduces an explicit latent variable that captures high-level trajectory intentions.
  </div>
  
  <div class="subsection-title">Joint Distribution</div>
  <div class="equation-box" style="text-align: center;">
      $$p_\theta(x, z) = p(z) \cdot p_\theta(x|z)$$
      <div style="margin-top: 1rem;">
          <strong>Prior:</strong> $z \sim \mathcal{N}(0, I_{64})$ where $z \in \mathbb{R}^{64}$
      </div>
  </div>
  
  <div class="text-content">
      The latent variable <strong>z</strong> serves as a compact representation of the entire trajectory's plan. By sampling from the prior during generation or inferring during execution, LAP can produce diverse but coherent catching behaviors.
  </div>
  
  <div class="subsection-title">Trajectory Generator</div>
  <div class="equation-box">
    $$p_\theta(x|z) = \prod_{t=1}^{T} p_\theta(a_t | o_{t-K:t}, a_{t-K:t-1}, z)$$
    <br>
    where $a_t \sim \mathcal{N}(g_\theta(\cdot), I_{|a|})$
  </div>
  
  <div class="text-content">
    The trajectory generator is implemented as a <strong>Causal Transformer</strong> that autoregressively predicts actions. Key architectural features include:
  </div>
  
  <div class="features-grid">
    <div class="feature-card">
        <h3>Cross-Attention to Latent</h3>
        <p>The Transformer decoder attends to the latent variable z at each layer, allowing the latent plan to guide action generation throughout the trajectory</p>
    </div>
    
    <div class="feature-card">
        <h3>History Context Window</h3>
        <p>Conditions on past K timesteps of observations and actions $(o_{t-K:t}, a_{t-K:t-1})$ to capture temporal dependencies</p>
    </div>
    
    <div class="feature-card">
        <h3>Gaussian Action Distribution</h3>
        <p>Outputs mean actions $g_\theta(\cdot)$ with fixed unit variance, enabling both deterministic execution and stochastic exploration</p>
    </div>
  </div>
  
  <div class="highlight" style="margin-top: 2rem;">
    <h3 style="color: #2774AE; margin-top: 0;">Key Advantages</h3>
    <ul style="line-height: 1.8;">
        <li><strong>Open-Loop Generation:</strong> Can generate complete trajectories by unrolling from the latent plan</li>
        <li><strong>Closed-Loop Adaptation:</strong> Can adapt online by replanning the latent variable based on new observations</li>
        <li><strong>Interpretability:</strong> The latent space provides interpretable dimensions of trajectory variation</li>
    </ul>
  </div>
  </section>

<!-- Classical Variational Bayes Section -->
<section class="section">
<h2 class="section-title">üìä Classical Variational Bayes</h2>

<div class="text-content">
  LAP is trained using the classical variational Bayes framework, which provides a principled approach to learning both the generative model and approximate posterior over latent variables.
</div>

<div class="subsection-title">Evidence Lower Bound (ELBO)</div>
<div class="equation-box">
  $$\mathcal{L}(\theta, \mu, \sigma) = \mathbb{E}_{q(z|x)}[\log p_\theta(x|z)] - D_{KL}(q(z|x) \| p(z))$$
  <br>
  where $q(z|x) = \mathcal{N}(\mu, \sigma^2)$ with diagonal covariance
</div>

<div class="text-content">
  The ELBO objective consists of two terms: a <strong>reconstruction term</strong> that encourages the model to explain observed trajectories, and a <strong>KL divergence term</strong> that regularizes the posterior to remain close to the prior.
</div>

<div class="subsection-title">Two-Stage Optimization</div>
<div class="text-content">
  Unlike Variational Autoencoders (VAEs) that use amortized inference with an encoder network, LAP employs <strong>direct optimization</strong> of latent variables:
</div>

<div class="two-column" style="margin: 2rem 0;">
  <div class="feature-card">
      <h3>Local Optimization (Fast Learning)</h3>
      <ul style="line-height: 1.8;">
          <li>Optimize $(\mu_i, \sigma_i)$ for each trajectory $i$</li>
          <li>16 gradient ascent steps on the ELBO</li>
          <li>Adapts latent plans to specific demonstrations</li>
          <li>Achieves more precise trajectory-specific representations</li>
      </ul>
  </div>
  
  <div class="feature-card">
      <h3>Global Optimization (Slow Learning)</h3>
      <ul style="line-height: 1.8;">
          <li>Update shared decoder parameters $\theta$</li>
          <li>Performed after all local optimizations</li>
          <li>Learns general patterns across all demonstrations</li>
          <li>Builds a universal trajectory generator</li>
      </ul>
  </div>
</div>

<div class="highlight">
  <h3 style="color: #2774AE; margin-top: 0;">Why Not VAE?</h3>
  <p style="line-height: 1.8;">
      Standard VAEs use an encoder network $q_\phi(z|x)$ for amortized inference, which approximates the posterior in a single forward pass. However, this can lead to less precise latent representations, especially for complex trajectories. LAP's direct optimization approach achieves <strong>more accurate posterior inference</strong> for each trajectory by dedicating multiple gradient steps to finding the optimal latent representation, at the cost of additional computation during training.
  </p>
</div>

<div class="text-content" style="margin-top: 2rem;">
  This two-stage optimization mirrors biological learning systems with complementary fast and slow learning rates, similar to the hippocampus-neocortex system in the brain.
</div>
</section>

<!-- Variational Replanning Section -->
<section class="section">
<h2 class="section-title">üîÑ Variational Replanning</h2>

<div class="text-content">
  At test time, LAP performs <strong>variational replanning</strong> to continuously adapt the latent plan based on newly observed trajectory segments. This enables real-time adaptation to unexpected box trajectories or environmental changes.
</div>

<div class="subsection-title">Bayesian Posterior Update</div>
<div class="equation-box" style="text-align: center;">
  $$q(z | x_{0:t+\Delta}) \propto q(z | x_{0:t}) \cdot p(x_{t+1:t+\Delta} | x_{0:t}, z)$$
</div>

<div class="text-content">
  The posterior update follows Bayes' rule: the new posterior is proportional to the previous posterior (prior belief) multiplied by the likelihood of the newly observed trajectory segment. This formulation naturally balances stability and adaptability.
</div>

<div class="subsection-title">Optimization Objective</div>
<div class="equation-box">
  $$\max_{\mu,\sigma} \mathbb{E}_{q_{t+\Delta}}[\log p_\theta(x_{t+1:t+\Delta} | x_{0:t}, z)] - D_{KL}(q_{t+\Delta} \| q_t)$$
</div>

<div class="features-grid">
  <div class="feature-card">
      <h3>Likelihood Term</h3>
      <p>Encourages the updated latent plan to explain the newly observed trajectory segment $x_{t+1:t+\Delta}$, improving prediction accuracy</p>
  </div>
  
  <div class="feature-card">
      <h3>KL Regularization</h3>
      <p>Acts as a trust region constraint, preventing drastic changes to the latent plan and ensuring smooth adaptation</p>
  </div>
</div>

<div class="subsection-title">Efficient Real-Time Implementation</div>
<div class="two-column" style="margin: 2rem 0;">
  <div class="feature-card">
      <h3>Update Frequency</h3>
      <ul style="line-height: 1.8;">
          <li><strong>Latent Replanning:</strong> 30Hz (every 33ms)</li>
          <li><strong>Action Generation:</strong> 100Hz (every 10ms)</li>
          <li><strong>Replanning Horizon:</strong> $\Delta = 10$ timesteps</li>
      </ul>
  </div>
  
  <div class="feature-card">
      <h3>Optimization Efficiency</h3>
      <ul style="line-height: 1.8;">
          <li><strong>Single Gradient Step:</strong> Fast update compared to 16 steps during training</li>
          <li><strong>Warm Start:</strong> Initialize from previous posterior</li>
          <li><strong>GPU Acceleration:</strong> Parallel batch processing</li>
      </ul>
  </div>
</div>

<div class="highlight">
  <h3 style="color: #2774AE; margin-top: 0;">The Role of KL Divergence</h3>
  <p style="line-height: 1.8;">
      The $D_{KL}(q_{t+\Delta} \| q_t)$ term is crucial for stable real-time adaptation. Without it, the latent plan could change drastically between updates, leading to jerky or inconsistent robot motions. The KL term enforces <strong>incremental adaptation</strong>, where the new plan remains close to the previous plan while gradually incorporating new information. This mirrors how humans smoothly adjust their catching strategy as they observe the ball's trajectory, rather than abruptly changing their motion plan.
  </p>
</div>

<div class="text-content" style="margin-top: 2rem;">
  This variational replanning framework enables LAP to achieve the responsiveness of reactive controllers while maintaining the smoothness and foresight of trajectory optimization methods.
</div>
</section>

<!-- Experimental Results Section -->
<section class="section">
<h2 class="section-title">üß™ Experimental Results</h2>

<div class="text-content">
  We evaluate LAP on box catching tasks across two different robot platforms, comparing against three baseline methods: model-based control, behavior cloning (BC), and diffusion policy. Our experiments demonstrate LAP's superior performance across multiple metrics.
</div>

<div class="subsection-title">Quantitative Results</div>
<div class="text-content">
  We evaluate all methods across four key metrics: success rate, energy consumption, motion smoothness, and adaptability to novel trajectories.
</div>

<div class="subsection-title">Comparative Videos</div>
<div class="text-content" style="margin-bottom: 1rem;">
  Watch how each method performs on the box catching task. Notice the differences in motion smoothness, energy efficiency, and adaptability.
</div>

<div class="experiments-section">
  <!-- LAP (Ours) -->
  <div class="method-section">
      <div class="method-header lap">LAP (Ours)</div>
      <div class="videos-grid">
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/lap-robot-A-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/lap-robot-A-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/lap-robot-A-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 3</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/lap-robot-B-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/lap-robot-B-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/lap-robot-B-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 3</div>
          </div>
      </div>
  </div>
  
  <!-- Model-Based -->
  <div class="method-section">
      <div class="method-header model-based">Model-Based</div>
      <div class="videos-grid">
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/mbp-robot-A-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/mbp-robot-A-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/mbp-robot-A-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 3</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/mbp-robot-B-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/mbp-robot-B-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/mbp-robot-B-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 3</div>
          </div>
      </div>
  </div>
  
  <!-- Behavior Cloning -->
  <div class="method-section">
      <div class="method-header bc">Behavior Cloning</div>
      <div class="videos-grid">
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/bc-robot-A-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/bc-robot-A-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/bc-robot-A-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 3</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/bc-robot-B-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/bc-robot-B-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/bc-robot-B-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 3</div>
          </div>
      </div>
  </div>
  
  <!-- Diffusion Policy -->
  <div class="method-section">
      <div class="method-header diffusion">Diffusion Policy</div>
      <div class="videos-grid">
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/df-robot-A-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/df-robot-A-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/df-robot-A-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot A - Box 3</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/df-robot-B-box-1.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 1</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/df-robot-B-box-2.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 2</div>
          </div>
          <div class="video-item">
              <div class="video-container">
                  <video controls muted playsinline loading="lazy">
                      <source src="videos/df-robot-B-box-3.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="video-caption">Robot B - Box 3</div>
          </div>
      </div>
  </div>
</div>

<div class="results-table">
  <table>
      <thead>
          <tr>
              <th colspan="5">Robot A Performance</th>
          </tr>
          <tr>
              <th>Method</th>
              <th>Success Rate</th>
              <th>Energy (J)</th>
              <th>Smoothness</th>
              <th>Adaptability</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td><strong>Model-based</strong></td>
              <td class="best-result">30/30</td>
              <td>74.99</td>
              <td>Medium</td>
              <td>Low</td>
          </tr>
          <tr>
              <td><strong>BC</strong></td>
              <td>26/30</td>
              <td>31.39</td>
              <td>Low</td>
              <td>Low</td>
          </tr>
          <tr>
              <td><strong>Diffusion</strong></td>
              <td>20/30</td>
              <td>42.53</td>
              <td>Medium</td>
              <td>Medium</td>
          </tr>
          <tr>
              <td><strong>LAP (Ours)</strong></td>
              <td class="best-result">29/30</td>
              <td class="best-result">11.47</td>
              <td class="best-result">High</td>
              <td class="best-result">High</td>
          </tr>
      </tbody>
  </table>
  
  <table style="margin-top: 1rem;">
      <thead>
          <tr>
              <th colspan="5">Robot B Performance</th>
          </tr>
          <tr>
              <th>Method</th>
              <th>Success Rate</th>
              <th>Energy (J)</th>
              <th>Smoothness</th>
              <th>Adaptability</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td><strong>Model-based</strong></td>
              <td class="best-result">30/30</td>
              <td>33.12</td>
              <td>Medium</td>
              <td>Low</td>
          </tr>
          <tr>
              <td><strong>BC</strong></td>
              <td>27/30</td>
              <td>15.82</td>
              <td>Low</td>
              <td>Low</td>
          </tr>
          <tr>
              <td><strong>Diffusion</strong></td>
              <td>24/30</td>
              <td>21.32</td>
              <td>Medium</td>
              <td>Medium</td>
          </tr>
          <tr>
              <td><strong>LAP (Ours)</strong></td>
              <td class="best-result">30/30</td>
              <td class="best-result">7.14</td>
              <td class="best-result">High</td>
              <td class="best-result">High</td>
          </tr>
      </tbody>
  </table>
</div>

<div class="subsection-title">Key Findings</div>
<div class="features-grid" style="grid-template-columns: repeat(2, 1fr);">
  <div class="feature-card">
      <h3>Model-Based Control</h3>
      <p><strong>Very stable performance with the highest success rates.</strong> However, not energy-efficient, consuming 5-6.5√ó more energy than LAP while achieving similar success rates.</p>
  </div>
  
  <div class="feature-card">
      <h3>Behavior Cloning</h3>
      <p><strong>Failed to properly learn arm-raising timing and impact-reducing retreat motions</strong> despite showing moderate success rates due to task simplicity. Produces jerky, locally-optimal motions.</p>
  </div>
  
  <div class="feature-card">
      <h3>Diffusion Policy</h3>
      <p><strong>Only learned final pose with fine success rates despite task simplicity,</strong> with highly noisy execution. Shows moderate adaptation but inconsistent performance.</p>
  </div>
  
  <div class="feature-card">
      <h3>üéØ Latent Adaptive Planner</h3>
      <p><strong>Successfully learned box-catching timing and impact-mitigating retreat motions</strong> while achieving high success rates with significantly lower energy usage than the model-based planner.</p>
  </div>
</div>

</section>

<!-- Robot Platforms Section -->
<section class="section">
<h2 class="section-title">Cross-Platform Transfer: Two Robot Embodiments</h2>

<div class="two-column">
  <div class="image-figure">
      <img src="images/robot_a.png" alt="Robot A">
      <div class="image-caption">Robot A Platform</div>
  </div>
  <div class="image-figure">
      <img src="images/robot_b.png" alt="Robot B">
      <div class="image-caption">Robot B Platform</div>
  </div>
</div>

<div class="text-content">
  We validated LAP on two robot platforms with different kinematic structures and dynamics. This cross-platform evaluation demonstrates the generalizability of our approach and the effectiveness of the model-based data regeneration pipeline in handling embodiment differences.
</div>
</section>

<!-- Conclusion Section -->
<section class="section">
<h2 class="section-title">‚ú® Conclusion</h2>

<div class="text-content">
  We presented Latent Adaptive Planner (LAP), a novel approach to dynamic manipulation that combines trajectory-level latent variable modeling with real-time variational replanning. Through extensive experiments on box catching tasks, we demonstrated that LAP achieves:
</div>

<div class="features-grid" style="margin: 2rem 0; grid-template-columns: repeat(2, 1fr);">
  <div class="feature-card">
      <h3>üéì Efficient Learning</h3>
      <p>Learns from human demonstrations through model-based data regeneration</p>
  </div>
  
  <div class="feature-card">
      <h3>‚ö° Energy Efficiency</h3>
      <p>5-6.5√ó reduction in energy consumption vs. model-based control</p>
  </div>
  
  <div class="feature-card">
      <h3>üéØ High Performance</h3>
      <p>98-100% success rate with smooth, human-like motions</p>
  </div>
  
  <div class="feature-card">
      <h3>üîÑ Real-Time Adaptation</h3>
      <p>Continuous replanning at 30Hz for robust generalization</p>
  </div>
</div>

<div class="text-content">
  The key insight of LAP is that explicit latent variable modeling enables efficient representation of trajectory-level intentions, while variational replanning provides a principled framework for real-time adaptation. This combination opens new possibilities for learning-based dynamic manipulation in unstructured environments.
</div>
</section>

<!-- Citation Section -->
<section class="section">
<h2 class="section-title">üìù Citation</h2>

<div class="citation">
  <button class="copy-btn" onclick="copyBibtex()">Copy</button>
  <pre id="bibtex">@inproceedings{noh2025latent,
title = {Latent Adaptive Planner for Dynamic Manipulation},
author = {Noh, Donghun and Kong, Deqian and Zhao, Minglu and Lizarraga, Andrew and Xie, Jianwen and Wu, Ying Nian and Hong, Dennis},
booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},
year = {2025}
}</pre>
</div>
</section>

</main>
</div>

<script>
function copyBibtex() {
const bibtex = document.getElementById('bibtex').textContent;
navigator.clipboard.writeText(bibtex).then(function() {
const btn = document.querySelector('.copy-btn');
const originalText = btn.textContent;
btn.textContent = 'Copied!';
btn.style.background = '#28a745';
setTimeout(() => {
  btn.textContent = originalText;
  btn.style.background = '#2774AE';
}, 2000);
});
}

// Add smooth scrolling for anchor links
document.querySelectorAll('a[href^="#"]').forEach(anchor => {
anchor.addEventListener('click', function (e) {
e.preventDefault();
const target = document.querySelector(this.getAttribute('href'));
if (target) {
  target.scrollIntoView({
      behavior: 'smooth',
      block: 'start'
  });
}
});
});
</script>
</body>
</html>