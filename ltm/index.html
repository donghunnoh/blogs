<!-- template.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Standard favicon -->
  <link rel="icon" type="image/x-icon" href="./favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png">

  <!-- Apple Touch Icons -->
  <link rel="apple-touch-icon" sizes="180x180" href="./apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes="152x152" href="./apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="120x120" href="./apple-touch-icon-120x120.png">

  <!-- Microsoft Tiles -->
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="./mstile-144x144.png">

  <!-- Web App Manifest -->
  <link rel="manifest" href="./site.webmanifest">

  <!-- Font Awesome (Open Source Icons) -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  <!-- SEO / Social meta -->
  <meta name="description" content="<p>We propose Latent Thought Models (LTMs) that add explicit ‚Äúlatent thought vectors‚Äù as internal abstract representations. Before generating text, LTMs first develop those internal thoughts, then use them to guide word-by-word generation. The model learns through a dual-rate process: fast learning that adapts thoughts for specific text and slow learning of general linguistic patterns. Compared to LLMs, LTMs achieve much better sample and computational efficiency. LTMs demonstrate in-context learning at a significantly smaller scale. Most importantly, LTMs introduce ‚Äúinference-time computation‚Äù as a new scaling axis beyond LLMs, potentially transforming how we build efficient and generalizable AI systems.</p>">
  <meta name="keywords" content="Inference-time computationLanguage modelsVariational BayesLatent variable model">
  <meta name="author" content="Deqian Kong">

  <!-- OpenGraph / Twitter -->
  <meta property="og:title" content="Latent Thought Models:">
  <meta property="og:description" content="<p>We propose Latent Thought Models (LTMs) that add explicit ‚Äúlatent thought vectors‚Äù as internal abstract representations. Before generating text, LTMs first develop those internal thoughts, then use them to guide word-by-word generation. The model learns through a dual-rate process: fast learning that adapts thoughts for specific text and slow learning of general linguistic patterns. Compared to LLMs, LTMs achieve much better sample and computational efficiency. LTMs demonstrate in-context learning at a significantly smaller scale. Most importantly, LTMs introduce ‚Äúinference-time computation‚Äù as a new scaling axis beyond LLMs, potentially transforming how we build efficient and generalizable AI systems.</p>">
  
  <link rel="canonical" href="https://arxiv.org/pdf/2502.01567">

  <title>Latent Thought Models:</title>

  <!-- MIT‚Äëlicensed font delivered via MIT‚Äëlicensed jsDelivr CDN (Fontsource). -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link href="https://cdn.jsdelivr.net/npm/@fontsource/figtree@5.0.14/index.min.css" rel="stylesheet" />

  <!-- Tailwind CSS + Typography plugin -->
  <script src="https://cdn.tailwindcss.com?plugins=typography"></script>
  <script>
    tailwind.config = {
      theme: {
        fontFamily: {
          sans: ['Figtree', '-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Helvetica Neue', 'Arial', 'Noto Sans', 'sans-serif'],
          serif: ['Figtree', 'Georgia', 'Times New Roman', 'Times', 'serif']
        },
        extend: {
          typography: ({ theme }) => ({
            DEFAULT: {
              css: {
                color: theme('colors.gray.700'),
                fontFamily: theme('fontFamily.sans').join(','),
                a: {
                  color: theme('colors.blue.600'),
                  textDecoration: 'none',
                  borderBottom: '1px solid ' + theme('colors.blue.200'),
                  transition: 'all 0.2s ease',
                  '&:hover': { 
                    color: theme('colors.blue.700'),
                    borderBottomColor: theme('colors.blue.600')
                  }
                },
                h1: {
                  fontFamily: theme('fontFamily.serif').join(','),
                  fontWeight: '700',
                  color: theme('colors.gray.900')
                },
                h2: {
                  fontFamily: theme('fontFamily.serif').join(','),
                  fontWeight: '700',
                  color: theme('colors.gray.900'),
                  marginTop: '2em',
                  marginBottom: '0.5em',
                  fontSize: '1.75rem'
                },
                h3: {
                  fontFamily: theme('fontFamily.serif').join(','),
                  fontWeight: '600',
                  color: theme('colors.gray.800'),
                  marginTop: '1.5em',
                  marginBottom: '0.5em',
                  fontSize: '1.35rem'
                },
                p: { 
                  marginBottom: '1.25em',
                  lineHeight: '1.7'
                },
                blockquote: {
                  fontStyle: 'italic',
                  color: theme('colors.gray.600'),
                  borderLeftColor: theme('colors.blue.200'),
                  paddingLeft: '1em',
                  backgroundColor: theme('colors.gray.50'),
                  borderRadius: '0.25rem',
                  margin: '1.5rem 0',
                  boxShadow: '0 1px 2px rgba(0,0,0,0.05)'
                },
                'h2#tldr + p': {
                  position: 'relative',
                  background: theme('colors.blue.50'),
                  borderLeft: '3px solid ' + theme('colors.blue.500'),
                  margin: '0.5rem 0 2.5rem',
                  padding: '1.75rem 2rem',
                  fontSize: '0.95rem',
                  lineHeight: '1.8',
                  color: theme('colors.gray.700'),
                  borderRadius: '0.3rem',
                  boxShadow: '0 2px 5px rgba(0,0,0,0.04)',
                },
                'h2#tldr': {
                  marginBottom: '1.5rem',
                  fontSize: '1.1rem',
                  fontWeight: '600',
                  color: theme('colors.white'),
                  backgroundColor: theme('colors.blue.600'),
                  display: 'inline-block',
                  padding: '0.3rem 1rem',
                  borderRadius: '9999px',
                  textTransform: 'uppercase',
                  letterSpacing: '0.05em',
                },
                code: {
                  backgroundColor: theme('colors.gray.100'),
                  padding: '0.2em 0.4em',
                  borderRadius: '0.25em',
                  fontSize: '0.9em',
                  border: '1px solid ' + theme('colors.gray.200')
                },
                pre: {
                  backgroundColor: theme('colors.gray.50'),
                  padding: '1.25em',
                  borderRadius: '0.5em',
                  overflowX: 'auto',
                  color: theme('colors.gray.800'),
                  border: '1px solid ' + theme('colors.gray.200'),
                  boxShadow: '0 2px 5px rgba(0,0,0,0.03)'
                },
                'pre code': {
                  backgroundColor: 'transparent',
                  border: 'none',
                  padding: '0'
                },
                'ul > li::marker': { color: theme('colors.blue.500') },
                'ol > li::marker': { color: theme('colors.gray.600') },
                strong: { color: theme('colors.gray.900'), fontWeight: '600' },
                table: {
                  fontSize: '0.9rem',
                  width: '100%',
                  marginTop: '2rem',
                  marginBottom: '2rem',
                  borderCollapse: 'separate',
                  borderSpacing: '0',
                  boxShadow: '0 2px 5px rgba(0,0,0,0.05)',
                  borderRadius: '0.375rem',
                  overflow: 'hidden'
                },
                'thead th': {
                  backgroundColor: theme('colors.gray.50'),
                  borderBottom: '1px solid ' + theme('colors.gray.200')
                },
                'tbody tr': {
                  borderBottom: '1px solid ' + theme('colors.gray.100'),
                  '&:last-child': { borderBottom: 'none' }
                }
              }
            }
          })
        }
      }
    }
  </script>
  
  <!-- Copy to clipboard functionality -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const copyButtons = document.querySelectorAll('.copy-button');
      
      copyButtons.forEach(button => {
        button.addEventListener('click', function() {
          const codeBlock = this.parentElement.nextElementSibling.querySelector('code');
          const textToCopy = codeBlock.innerText;
          
          navigator.clipboard.writeText(textToCopy).then(() => {
            // Change button text temporarily
            const originalText = this.innerText;
            this.innerText = 'Copied!';
            this.classList.add('copied');
            
            setTimeout(() => {
              this.innerText = originalText;
              this.classList.remove('copied');
            }, 2000);
          });
        });
      });
    });
  </script>
  
  <style>
    .copy-button {
      position: absolute;
      top: 0.5rem;
      right: 0.5rem;
      padding: 0.25rem 0.75rem;
      background-color: rgba(255, 255, 255, 0.9);
      border: 1px solid #e2e8f0;
      border-radius: 0.25rem;
      font-size: 0.75rem;
      font-weight: 500;
      color: #4a5568;
      cursor: pointer;
      transition: all 0.2s ease;
      z-index: 10;
    }
    
    .copy-button:hover {
      background-color: #f7fafc;
      border-color: #cbd5e0;
    }
    
    .copy-button.copied {
      background-color: #e6ffed;
      border-color: #a3e635;
      color: #15803d;
    }
    
    .code-block-wrapper {
      position: relative;
    }
  </style>
  
  <!-- LiveReload script (will only connect when the LiveReload server is running) -->
    <script>
    document.write('<script src="http://' + (location.host || 'localhost').split(':')[0] + ':8000/livereload.js?snipver=1"></' + 'script>')
  </script>
  </head>
<body class="text-gray-700 font-sans">

  <!-- Header -->
  <header class="bg-white shadow-sm py-8 mb-12">
    <div class="mx-auto max-w-3xl px-4 text-center">
      <h1 class="text-4xl font-serif font-bold text-gray-900">Latent Thought Models:</h1>
      <p class="mt-2 text-2xl text-gray-900">Language models with explicit latent thought vectors and variational Bayes inference-time computation</p>

      <!-- Author list with affiliation superscripts -->
      <p class="mt-4 text-gray-600 text-lg flex justify-center flex-wrap gap-x-4 gap-y-0.5">
                              <span class="whitespace-nowrap">
                              <a href="https://sites.google.com/view/deqiankong/" class="text-blue-600 hover:underline -mr-1">Deqian Kong</a>
                            <sup>12*</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://mingluzhao.github.io" class="text-blue-600 hover:underline -mr-1">Minglu Zhao</a>
                            <sup>1*</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://dehongxu.github.io" class="text-blue-600 hover:underline -mr-1">Dehong Xu</a>
                            <sup>1*</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://bpucla.github.io" class="text-blue-600 hover:underline -mr-1">Bo Pang</a>
                            <sup>3</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://scholar.google.com/citations?user=uPk5l1EAAAAJ&amp;hl=en" class="text-blue-600 hover:underline -mr-1">Shu Wang</a>
                            <sup>1</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="http://www.stat.ucla.edu/~edouardohonig/" class="text-blue-600 hover:underline -mr-1">Edouardo Honig</a>
                            <sup>1</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://www.kungfu.ai/person/zz-si" class="text-blue-600 hover:underline -mr-1">Zhangzhang Si</a>
                            <sup>4</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://scholar.google.com/citations?user=hoZesOwAAAAJ&amp;hl=en" class="text-blue-600 hover:underline -mr-1">Chuan Li</a>
                            <sup>2</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="http://www.stat.ucla.edu/~jxie/" class="text-blue-600 hover:underline -mr-1">Jianwen Xie</a>
                            <sup>2**</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="https://siruixie.com" class="text-blue-600 hover:underline -mr-1">Sirui Xie</a>
                            <sup>1**</sup>
            </span> 
                      <span class="whitespace-nowrap">
                              <a href="http://www.stat.ucla.edu/~ywu/research.html" class="text-blue-600 hover:underline -mr-1">Ying Nian Wu</a>
                            <sup>1**</sup>
            </span>              </p>

      <!-- Affiliations list -->
            <div class="mt-2 text-gray-600 text-md flex gap-x-2 justify-center flex-wrap">
                  <span><sup>1</sup> UCLA</span>
                  <span><sup>2</sup> Lambda, Inc.</span>
                  <span><sup>3</sup> Salesforce Research</span>
                  <span><sup>4</sup> KUNGFU.AI</span>
                  <span><sup>*</sup> Equal Contribution</span>
                  <span><sup>**</sup> Equal Advising</span>
              </div>
      
            <div class="mt-4 flex justify-center flex-wrap gap-3 sm:mx-10 md:mx-18">
                  <a href="https://arxiv.org/pdf/2502.01567" class="px-5 py-2 bg-gray-200 text-gray-800 rounded-full hover:bg-gray-300 transition flex items-center gap-2 shadow-sm">
                          <i class="fas fa-file-alt"></i>
                        Paper (arXiv)
          </a>
                  <a href="" class="px-5 py-2 bg-gray-200 text-gray-800 rounded-full hover:bg-gray-300 transition flex items-center gap-2 shadow-sm">
                          <i class="fas fa-code"></i>
                        Code (GitHub)
          </a>
              </div>
          </div>
  </header>

  <!-- Main content -->
  <main class="mx-auto max-w-3xl px-4 mb-16">
    <article class="prose prose-lg mx-auto bg-white p-8">
      <div class="text-center -mt-20 text-gray-600">
      <p>Jump to: <a href="#tldr">TL;DR</a>, <a href="#introduction">Introduction</a>, <a href="#key-findings">Key Findings</a>, <a href="#conclusion">Conclusion</a>, <a href="#acknowledgements">Acknowledgements</a>, <a href="#related-links">Related Links</a>, <a href="#bibtex">Cite</a></p>
      </div>
      <h2 id="tldr">TL;DR</h2>
      <p>We introduce Latent Thought Models (LTMs), a novel class of language models that incorporate explicit <strong>latent thought vectors</strong> following a prior model in latent space. LTMs use a dual-rate optimization process within the variational Bayes framework: fast inference-time computation for latent vectors and slow learning of decoder parameters. This approach achieves superior sample and parameter efficiency compared to autoregressive models and introduces <strong>inference-time computation</strong> as a new scaling dimension beyond traditional LLMs.</p>
      <div class="mt-10" style="max-width: 700px; margin: 0 auto;">
      <img 
          src="images/model.png" 
          alt="High-Level Overview: LTMs first develop internal latent thoughts, then use them to guide autoregressive text generation through a Transformer decoder." 
          style="width: 70%; height: auto; display: block; margin: 0 auto;"
        />
      <p style="margin-top: 8px; font-size: 14px; color: #555; text-align: left;">
      High-Level Overview: LTMs first develop internal latent thoughts vectors ùëß, then use them to guide autoregressive text generation ùë• through a Transformer decoder.
      </p>
      </div>
      <h2 id="introduction">Introduction</h2>
      <p>Current language models scale primarily through increasing parameters and training data, leaving inference-time computation largely unexplored. We introduce Latent Thought Models (LTMs) that incorporate explicit ‚Äúthinking‚Äù before generation (or speaking).</p>
      <p><strong>Key Inspirations</strong>:</p>
      <ol type="1">
      <li><p><strong>Declarative vs.¬†Procedural Memory</strong>: Latent vectors parallel declarative/episodic memory with fast learning, while global decoder parameters mirror procedural memory with slow learning;</p></li>
      <li><p><strong>Complementary Learning Systems: Fast and Slow</strong>: Our dual-rate learning mirrors the hippocampus (rapid learning of specific experiences) and neocortex (slower learning of general knowledge).</p></li>
      <li><p><strong>Language of Thought</strong>: Latent vectors serve as ‚Äúwords‚Äù in an internal cognitive language‚Äîa ‚Äúmentalese‚Äù that underlies our ability to learn and use natural languages, realizing a computational ‚Äúthink before speak‚Äù paradigm.</p></li>
      </ol>
      <p><strong>Why This Matters</strong>: LTMs unlock inference-time computation as a new scaling dimension‚Äîthe process of finding better internal representations (posterior distributions of latent thought vectors). Just as humans can achieve better understanding by ‚Äúthinking harder‚Äù about a problem, LTMs can use more inference-time computation to achieve better performance with significantly less training data and computation.</p>
      <h2 id="key-findings">Key Findings</h2>
      <p>Our empirical studies reveal several important discoveries about LTMs‚Äô unique scaling properties and capabilities:</p>
      <ol type="1">
      <li><strong>Scaling Behaviors of Inference-Time Computation</strong> LTMs demonstrate a new scaling dimension beyond traditional model parameters. Performance consistently improves with more inference steps, as the model iteratively refines latent thought vectors to find better internal representations.</li>
      </ol>
      <div class="md:mx-10">
      <figure>
      <img src="images/ppl_val_3.png" alt="" /><figcaption><strong>Inference-Time Scaling</strong>: Performance improvement as a function of inference steps and latent size, demonstrating the new scaling dimension introduced by LTMs.</figcaption>
      </figure>
      </div>
      <ol start="2" type="1">
      <li><strong>Sample and Computational Efficiency</strong> LTMs achieve superior efficiency by leveraging inference steps and latent size to improve performance more effectively than simply scaling model parameters or training data.</li>
      </ol>
      <!-- <div class="md:mx-5 grid grid-cols-1 md:grid-cols-2 gap-4">
      ![Sample Efficiency](images/scaling_tokens_new.png)

      ![Computational Efficiency](images/scaling_flops_new.png)
      </div> -->
      <div class="md:mx-20">
      <figure>
      <img src="images/scaling_tokens_new.png" alt="" /><figcaption>Sample Efficiency</figcaption>
      </figure>
      </div>
      <div class="md:mx-20">
      <figure>
      <img src="images/scaling_flops_new.png" alt="" /><figcaption>Computational Efficiency</figcaption>
      </figure>
      </div>
      <ol start="3" type="1">
      <li><strong>Emergent In-Context Learning in Mathematics</strong> LTMs demonstrate emergent few-shot mathematical reasoning capabilities at remarkably small scales. The explicit latent thought modeling enables mathematical reasoning to emerge much earlier in model scaling than traditional approaches.</li>
      </ol>
      <div class="md:mx-20">
      <figure>
      <img src="images/gsm8k.png" alt="" /><figcaption><strong>Mathematical Reasoning</strong>: Emergent in-context learning capabilities for mathematical tasks at small model scales. (LTM-L has 76M parameters.)</figcaption>
      </figure>
      </div>
      <h2 id="conclusion">Conclusion</h2>
      <p>Latent Thought Models represent a significant advancement in language modeling by introducing explicit latent thought vectors and inference-time computation as a new scaling dimension. The dual-rate optimization within the variational Bayes framework enables superior sample and parameter efficiency while maintaining competitive generation quality.</p>
      <p>This work opens new directions for efficient language model design and suggests that explicit modeling of internal representations can unlock additional scaling dimensions beyond traditional approaches. The ability to trade model size for inference computation provides flexible deployment strategies for resource-constrained environments.</p>
      <h2 id="acknowledgements">Acknowledgements</h2>
      <p>We thank Ruiqi Gao and Kevin Murphy from Google DeepMind for insightful discussions and valuable suggestions. Y. W. was partially supported by NSF DMS-2015577, NSF DMS-2415226, and a gift fund from Amazon. We gratefully acknowledge the support of <a href="https://lambda.ai">Lambda, Inc.</a> for providing the compute for this project.</p>
      <h2 id="related-links">Related Links</h2>
      <h2 id="bibtex">BibTeX</h2>
      <p>If you consider citing us, feel free to use the bibtex-entry below.</p>
      <div class="code-block-wrapper">
      <p><button class="copy-button">Copy</button></p>
      <div class="sourceCode" id="cb1"><pre class="sourceCode bibtex"><code class="sourceCode bibtex"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="va">@article</span>{<span class="ot">kong2025latent</span>,</span>
      <span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>  <span class="dt">title</span> = {Latent Thought Models with Variational Bayes Inference-Time Computation},</span>
      <span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>  <span class="dt">author</span> = {Kong, Deqian and Zhao, Minglu and Xu, Dehong and Pang, Bo and Wang, Shu and Honig, Edouardo and Si, Zhangzhang and Li, Chuan and Xie, Jianwen and Xie, Sirui and Wu, Ying Nian},</span>
      <span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>  <span class="dt">booktitle</span> = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},</span>
      <span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>  <span class="dt">year</span> = {2025}</span>
      <span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>}</span></code></pre></div>
      </div>
    </article>
  </main>

  <!-- Footer -->
  <footer class="bg-white py-6 border-t border-gray-100 shadow-inner">
    <div class="mx-auto max-w-3xl px-6 text-center">
      <div class="mb-2">
                <div class="flex justify-center flex-wrap gap-2 mb-4">
                      <a href="https://arxiv.org/pdf/2502.01567" class="text-gray-500 hover:text-blue-500 transition-colors duration-200 mx-2">
                              <i class="fas fa-file-alt"></i>
                          </a>
                      <a href="" class="text-gray-500 hover:text-blue-500 transition-colors duration-200 mx-2">
                              <i class="fas fa-code"></i>
                          </a>
          	      <a href="https://librecounter.org/referer/show" class="text-gray-500 hover:text-blue-500 transition-colors duration-200 mx-2" target="_blank">
			<img src="https://librecounter.org/outline-orange.svg" referrerPolicy="unsafe-url" width="24"/>
	      </a>
        </div>
              </div>
      <div class="text-gray-500 text-sm">
        &copy; 2025 Deqian Kong.
                  Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="text-blue-600 hover:underline transition-colors duration-200">CC BY-SA 4.0</a>.
              </div>
    </div>
  </footer>

</body>
</html>
